{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLM-RL Training with LoRA\n",
    "\n",
    "Quick training setup for Qwen3-VL with LoRA fine-tuning on PPO."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Setup Dependencies\n\nInstall required packages for Colab environment:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Install specific transformers version required for Qwen3-VL\n!pip install -q transformers==4.57.1\n\n# Ensure we are on the 'lora' branch and using the local repo version of vlmrl\nimport os, inspect, sys, subprocess
repo_root = os.path.abspath(os.getcwd())\nprint(f'Repo root: {repo_root}')\n\nif 'COLAB_GPU' in os.environ:\n    # On Colab, clone specific branch and install editable
    !git clone -b lora https://github.com/sdan/vlm-gym.git || true\n    %cd vlm-gym\n    !git rev-parse --abbrev-ref HEAD\n    !pip uninstall -y -q vlmrl || true\n    !pip install -q -e .\nelse:\n    # Local dev: if this is a git repo, switch to lora branch
    if os.path.exists('.git'):
        print('Ensuring branch: lora')
        _ = subprocess.run(['git','fetch','origin'], check=False)
        _ = subprocess.run(['git','checkout','lora'], check=False)
        _ = subprocess.run(['git','pull','--ff-only'], check=False)
        os.system('git rev-parse --abbrev-ref HEAD')
    else:
        print('Not a git repo here; skipping branch switch.')

    # Force a clean editable install from this repo
    print('Reinstalling vlmrl from local repo (editable)')
    !pip uninstall -y -q vlmrl || true\n    !pip install -q -e .\n\n# Sanity check: confirm trainer path resolves to this repo
import importlib, inspect as _inspect
try:
    import vlmrl.core.train as _t
    print('Trainer module path:', _inspect.getsourcefile(_t))
except Exception as e:
    print('Could not import trainer after install:', e)
\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# Check working directory\nimport os\nprint(f\"Working directory: {os.getcwd()}\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Convert HF Checkpoint to JAX\n\nDownload and convert Qwen3-VL checkpoint from HuggingFace to JAX format for LoRA training:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Convert Qwen3-VL-2B from HuggingFace to JAX format\n!python -m vlmrl.utils.hf_to_jax \\\n    --model_type qwen3vl \\\n    --hf_repo Qwen/Qwen3-VL-2B-Instruct \\\n    --model_dir checkpoints/qwen3vl_2b\n\nprint(\"âœ“ Checkpoint converted and ready for LoRA training\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Training Configuration\n\nThis notebook runs a quick training session with:\n- **Model**: Qwen3-VL-2B-Instruct\n- **LoRA**: Enabled (rank=16, alpha=32, lr_mult=10.0)\n- **Training**: 50 steps, batch_size=4, PPO epochs=1\n- **Optimizer**: AdamW with lr=1e-6\n- **W&B**: Offline mode"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# Training parameters\nconfig = {\n    'model_dir': 'checkpoints/qwen3vl_2b',\n    'wandb_mode': 'offline',\n    'total_steps': 50,\n    'batch_size': 4,\n    'ppo_epochs': 1,\n    'learning_rate': 1e-6,\n    'optimizer': 'adamw',\n    'lora_enable': 1,\n    'lora_rank': 16,\n    'lora_alpha': 32,\n    'lora_lr_mult': 10.0,\n}\n\n# Build command\ncmd_parts = ['python', '-m', 'vlmrl.core.train']\nfor key, value in config.items():\n    cmd_parts.append(f'--{key}')\n    cmd_parts.append(str(value))\n\ncommand = ' '.join(cmd_parts)\nprint(\"Training command:\")\nprint(command)",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training\n",
    "\n",
    "Execute the training run. This will:\n",
    "1. Load the Qwen3-VL model from checkpoint\n",
    "2. Initialize LoRA adapters\n",
    "3. Run PPO training for 50 steps\n",
    "4. Save checkpoints to the runs directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# Run training - use = instead of space for flag values\n!python -m vlmrl.core.train \\\n    --model_dir=checkpoints/qwen3vl_2b \\\n    --wandb_mode=offline \\\n    --total_steps=50 \\\n    --batch_size=4 \\\n    --ppo_epochs=1 \\\n    --learning_rate=1e-6 \\\n    --optimizer=adamw \\\n    --lora_enable=1 \\\n    --lora_rank=16 \\\n    --lora_alpha=32 \\\n    --lora_lr_mult=10.0",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor Training\n",
    "\n",
    "Check the training outputs and W&B logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# List saved checkpoints\n",
    "!ls -lh runs/"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# View W&B offline logs\n",
    "!ls -lh wandb/"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Sample Rollout\n\nTest the trained model with a few sample episodes:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Run sample rollouts on the geospot environment\n!python -m vlmrl.core.rollout \\\n    --model_dir checkpoints/qwen3vl_2b \\\n    --env_name geospot \\\n    --episodes 5 \\\n    --batch_size 1 \\\n    --temperature 0.7 \\\n    --top_p 0.9 \\\n    --max_new_tokens 64",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
